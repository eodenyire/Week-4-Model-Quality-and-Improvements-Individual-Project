# -*- coding: utf-8 -*-
"""Tuesday week 4 : Model Quality and Improvements - Independent Project-Emmanuel Odenyire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ymM5ANuByrAg6s0jsc-l19r3e04R_BaJ

#Model Quality and Improvements - Independent Project

##Problem Statement

#As a data professional working for a pharmaceutical company, you need to develop a model that predicts whether a patient will be diagnosed with diabetes. The model needs to have an accuracy score greater than 0.85.
#I am required to document the following steps during the analysis:
#● Data Importation
#● Data Exploration
#● Data Cleaning
#● Data Preparation
#● Data Modeling (Using Decision Trees, Random Forest and Logistic Regression)
#● Model Evaluation
#● Hyparameter Tuning
#● Findings and Recommendations

##Data Importation
"""

# Importing the necessary libraries 
import pandas as pd

# Dataset URL: https://bit.ly/DiabetesDS

diabetics_diagnosis_df = pd.read_csv("https://bit.ly/DiabetesDS")

diabetics_diagnosis_df.head()

"""##Data Exploration"""

# Checking the dataset size for data exploration using .shape function
diabetics_diagnosis_df.shape

# Checking null shapes in the dataset
diabetics_diagnosis_df.isna().sum()

# Describing the Data in the dataset
diabetics_diagnosis_df.info()

# Summary statistics for the data set
diabetics_diagnosis_df.describe()

#See the amount of data on each target
diabetics_diagnosis_df['Outcome'].value_counts()

# Expressing the outcome as a percentage
print('1. The percentage of patients without diabetes are ' 
      + str(round(((diabetics_diagnosis_df["Outcome"].isin([0]).sum())/diabetics_diagnosis_df.shape[0])*100,2)) + ' %')
print('2. The percentage of patients with diabetes are ' 
      + str(round(((diabetics_diagnosis_df["Outcome"].isin([1]).sum())/diabetics_diagnosis_df.shape[0])*100,2)) + ' %')

"""##Data Cleaning"""

# Checking column names
diabetics_diagnosis_df.columns

# Standardizing a dataset by stripping leading and trailing spaces and setting all columns to lower
diabetics_diagnosis_df.columns = diabetics_diagnosis_df.columns.str.strip().str.lower()
diabetics_diagnosis_df.columns

# Checking for missing values in the dataset
diabetics_diagnosis_df.isna().sum()

"""There are no missing values in the data set"""

# Checking for duplicates
diabetics_diagnosis_df.duplicated().sum()

"""There are no duplicates in the data

##Data Preparation
"""

# checking correlation of features and target
import matplotlib.pyplot as plt
import seaborn as sns

features = diabetics_diagnosis_df.columns
corr_= diabetics_diagnosis_df[features].corr()
plt.figure(figsize=(12,8))
sns.heatmap(corr_, annot=True, fmt = ".2f", cmap = "coolwarm");

# Plotting Histogram for features to show relationship between them and target 
for feature in features[:-1]:
  plt.hist(diabetics_diagnosis_df[diabetics_diagnosis_df['outcome']==1][feature], color= 'blue', alpha = 0.7, label = 'Diabetic', density=True)
  plt.hist(diabetics_diagnosis_df[diabetics_diagnosis_df['outcome']==0][feature], color= 'red', alpha = 0.7, label = 'Not Diabetic', density=True)
  plt.title(feature)
  plt.ylabel('Probability')
  plt.xlabel(feature)
  plt.legend()
  plt.show()

"""**Conclusion**
In the first graph, an increase in the features and targets affects both the probability of Diabetic and Non-diabetics. This has been evident in all the remaining graphs where an increase in the features and targets affects both the two variables in the relationship

##Data Modeling
"""

# We start data modeling by checking the first five records using .head function
diabetics_diagnosis_df.head()

#(Using Decision Trees, Random Forest and Logistic Regression)
#define features and target

#importing the necessary functions from sklearn needed 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split

features = diabetics_diagnosis_df.drop(['outcome'], axis=1)
target = diabetics_diagnosis_df['outcome']

x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=142)

# Training our models models
desc_model = DecisionTreeClassifier(random_state=12345)
forest_model = RandomForestClassifier(random_state=12345)
regression_model = LogisticRegression(random_state=12345, solver='liblinear')

desc_model.fit(x_train, y_train)
forest_model.fit(x_train, y_train)
regression_model.fit(x_train, y_train)

# Making our predictions
desc_test_predictions =desc_model.predict(x_test)
forest_test_predictions =forest_model.predict(x_test)
regression_test_predictions =regression_model.predict(x_test)

"""##Model Evaluation"""

# Calculating accuracy score
# Checking the accuracy of our model
from sklearn.metrics import accuracy_score 
print(f'DecisionTreeClassifier accuracy: {accuracy_score(y_test, desc_test_predictions)}')
print(f'RandomForestClassifier accuracy: {accuracy_score(y_test, forest_test_predictions)}')
print(f'LogisticRegression accuracy: {accuracy_score(y_test, regression_test_predictions)}')

from sklearn.metrics import classification_report
# Printing classification report for DecisionTreeClassifier
print(f'DecisionTreeClassifier classification report:\n {classification_report(y_test, desc_test_predictions)}')

print(f'RandomForestClassifier classification report:\n {classification_report(y_test, forest_test_predictions)}')

print(f'LogisticRegression classification report:\n {classification_report(y_test, regression_test_predictions)}')

"""##Hyparameter Tuning"""

# Tuning the hyperparameters for Decision tree classifier 
for depth in range(1, 20):
    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)
    model.fit(x_train, y_train)
    predictions_valid = model.predict(x_test)
    print('max_depth =', depth, ': ', end='')
    print(accuracy_score(y_test, predictions_valid))

# The best maximum depth is 2 so far
model = DecisionTreeClassifier(random_state=12345, max_depth=2)
model.fit(x_train, y_train)
predictions_valid = model.predict(x_test)
print(accuracy_score(y_test, predictions_valid))

# Tuning the hyperparameters for RandomForestClassifier()

def get_estimators():
  score = 0
  for estimators in range(1, 51):
      model = RandomForestClassifier(random_state=12345, n_estimators=estimators)
      model.fit(x_train, y_train)
      predictions_valid = model.predict(x_test)
      pred_score = accuracy_score(y_test, predictions_valid)
      if pred_score > score: 
          score = pred_score
  return print('n_estimators =', estimators, 'accuracy: ', score)

get_estimators()

# Tuning the hyperparameters for LogisticRegression
# You don't need to tune the hyperparameters of logistic regression. Just train it.
model = LogisticRegression(random_state=12345, solver='liblinear')
model.fit(x_train, y_train)
predictions_valid = model.predict(x_test)
print(f'accuracy score is: {accuracy_score(y_test, predictions_valid)}')

"""##Findings and Recommendations

From this analysis, logistic regression has given out the best and highestscore as compared to others, thus we can make a conclusion on the finding that logistic regression is the best model here
"""